{
  
    
        "post0": {
            "title": "Pyspark in Google Colab",
            "content": "Ever want to test out Apache spark without spinning up a linux box? Try out Colab! . The entire colab runs in a cloud VM. Let&#39;s investigate the VM. You will see that the current colab notebook is running on top of Ubuntu 18.04.6 LTS (at the time of this writing.) . !cat /etc/*release . DISTRIB_ID=Ubuntu DISTRIB_RELEASE=18.04 DISTRIB_CODENAME=bionic DISTRIB_DESCRIPTION=&#34;Ubuntu 18.04.6 LTS&#34; NAME=&#34;Ubuntu&#34; VERSION=&#34;18.04.6 LTS (Bionic Beaver)&#34; ID=ubuntu ID_LIKE=debian PRETTY_NAME=&#34;Ubuntu 18.04.6 LTS&#34; VERSION_ID=&#34;18.04&#34; HOME_URL=&#34;https://www.ubuntu.com/&#34; SUPPORT_URL=&#34;https://help.ubuntu.com/&#34; BUG_REPORT_URL=&#34;https://bugs.launchpad.net/ubuntu/&#34; PRIVACY_POLICY_URL=&#34;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&#34; VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic . !sudo apt-get -y install openjdk-8-jdk-headless . Reading package lists... Done Building dependency tree Reading state information... Done openjdk-8-jdk-headless is already the newest version (8u342-b07-0ubuntu1~18.04). The following package was automatically installed and is no longer required: libnvidia-common-460 Use &#39;sudo apt autoremove&#39; to remove it. 0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded. . In order to install Apache Spark on Linux based Ubuntu, access Apache Spark Download site and go to the Download Apache Spark section and click on the link from ordered list item number 3, this takes you to the page with mirror URL’s to download. copy the link from one of the mirror site. . !wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz . --2022-08-26 20:10:38-- https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644 Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 299321244 (285M) [application/x-gzip] Saving to: ‘spark-3.3.0-bin-hadoop3.tgz.1’ spark-3.3.0-bin-had 100%[===================&gt;] 285.45M 58.6MB/s in 6.5s 2022-08-26 20:10:45 (43.8 MB/s) - ‘spark-3.3.0-bin-hadoop3.tgz.1’ saved [299321244/299321244] . !tar xf spark-3.3.0-bin-hadoop3.tgz . !pip install -q findspark . OS module in Python provides functions for interacting with the operating system. OS comes under Python’s standard utility modules. This module provides a portable way of using operating system dependent functionality. . os.environ in Python is a mapping object that represents the user’s environmental variables. It returns a dictionary having user’s environmental variable as key and their values as value. . os.environ behaves like a python dictionary, so all the common dictionary operations like get and set can be performed. We can also modify os.environ but any changes will be effective only for the current process where it was assigned and it will not change the value permanently. . import os os.environ[&quot;JAVA_HOME&quot;] = &quot;/usr/lib/jvm/java-8-openjdk-amd64&quot; os.environ[&quot;SPARK_HOME&quot;] = &quot;/content/spark-3.3.0-bin-hadoop3&quot; . PySpark isn&#39;t on sys.path by default, but that doesn&#39;t mean it can&#39;t be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or adding pyspark to sys.path at runtime. findspark does the latter. . To initialize PySpark, just call . import findspark findspark.init() . To verify the automatically detected location, call . findspark.find() . &#39;/content/spark-3.3.0-bin-hadoop3&#39; . Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark. . You can give a name to the session using appName() and add some configurations with config() if you wish. . from pyspark.sql import SparkSession spark = (SparkSession .builder .master(&quot;local&quot;) .appName(&quot;my_colab_spark_app&quot;) .config(&#39;spark.ui.port&#39;, &#39;4050&#39;) .getOrCreate()) . Finally, print the SparkSession variable. . spark . SparkSession - in-memory . SparkContext . Spark UI . Version v3.3.0 Master local AppName my_colab_spark_app",
            "url": "https://nuttnice187.github.io/the-inception/2022/08/26/Spark_by_Example.html",
            "relUrl": "/2022/08/26/Spark_by_Example.html",
            "date": " • Aug 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Custom Data Structures, a Preface to Graph Neural Networks",
            "content": "So far in previous ML posts, we&#39;ve customized an algorihim to work with the structure offered by numpy arrays representing matrices. Depending on the algorithim applied, numpy arrays may not be the most efficient structure to work with. . What are we talking about here? Data Structures are a specialized means of organizing and storing data in computers in such a way that we can perform operations on the stored data more efficiently. . Early in my coding career, I discovered that in the programming world, we abstract data in a structure that can be represented as class object nodes. Specifically, while on assignment at Bank of America Merchant services by direction of my superior, I did some deep diving into MS SQL online transactional processing relational database engines and how they work. Internally, this particular product would rely upon applying algorithims to a data structure called a B-tree. Shout out to Itzik Ben-Gan, who wrote some literature that became foundational to my understanding of relational database management engines. My boss at BAMS happened to be involved with the technical review of Itzik&#39;s popular literature. These books were strongly recommended during my project onboarding. I have these books close by to date via Kindle. . B-trees as an astraction of data, presented themselves to me as difficult to comprehend. Before learning of them, my world consisted of tables or views: database objects that we may imagine as behaving like a spreadsheet. The table object in my in my mind&#39;s eye was revealed as an abstract way to view B-trees, underneath. . These B-tree structures and the algorithims that are applied to them are abstracted away from the analyst or administrator working in the database by means of structured query language or SQL. This opened up the natural observation that it would be critical for me to understand more about data structures if I want to go a layer deeper than the SQL abstractions. All of the sudden, there was more to the universe than user defined functions, procedures, and table objects. B-trees and their &quot;pages&quot; are a variant of binary search trees, a node-based binary tree data structure. . Scope . In this post we&#39;ll illustrate our knowledge of common data structures, how to work with them. Our aim here is to level set some understanding that we&#39;ll need before jumping into the topic of graph neural networks. You see, graphs are their own classification of custom data structure and graph neural networks, or GNN&#39;s have some interesting applications in the ML world. . Where to begin? My knowledge is finite. In honesty, after satisfying my initial curiousity, the subject would be revisted in very few instances in my career. Outside of the deprecated queue, I have hardly see custom structures in production during my career. In 2018, I learned how to build an algorithim that functions in traversing a linked list in fear that it would come up one day in a technical interview. To this day, it never has. . To ramp up the basics of data structures, we should discuss . linked lists, | binary search trees, | and graphs. | With an understanding of these three data structures, our readers should be able to follow subsequent discussions around GNN&#39;s. Let&#39;s state that as our goal. One of the objectives of this blog is to demonstrate my knowledge and transfer what I know to those who care to learn more. . Other common data structures include arrays, hash tables. These are variants of python lists and dictionaries. I&#39;m not going to spend time on those here. We&#39;ve spent some time working with them in previous posts, already. Remember we just want the bottom line up front when it comes to the mysterious prerequisites for working with GNN&#39;s. I will say before learning minimal nuances, my experiences in working with arrays and hash table like structures are rooted in JavaScript front end development and backend development with Django, .NET, nodeJS. To that end, I was first exposed to hash tables before gaining object oriented experience by way of witnessing the HASH JOIN SQL operation at BAMS and following up with Itzik&#39;s literature. . In common practices, detailed understanding data structures and algorithms are abstracted away through the use of data science libraries like sci kit learn. I figure if we want to get to the good stuff, we need to at least cover the basics for how some of these internal mechanics of ML training and prediction processes function. Let&#39;s also state the application of data structures and algorithims extend far beyond machine learning. . Data Generation . If data structures are specialized means of organizing data in memory or storage, such that we can work with them more efficiently, we need to start our journey by producing a dataset. I am a student of Mathematics: as previous member of an educational institution, yes, but also in life beyond school. . A sequence of data points or numbers that has always intrigued me includes prime numbers. For the sake of having data values to work with, let&#39;s create an algorithim that functions in returning the first n members of the prime number sequence. . Given . A prime number is a whole number greater than 1 whose only factors are 1 and itself. A factor is a whole number that can be divided evenly into another number. The first few prime numbers are 2, 3, 5, 7, 11, 13, 17, 19, 23 and 29. . Implementation . This function could easily be implemented as a method of a custom class object acting as an instantiation of a custom data structure. . def genPrimeLi(n=10): try: assert n &gt; 0, &quot;number of elements must be greater than zero&quot; assert isinstance(n, int), &quot;number of elements must be an integer&quot; seq = [] seq.append(2) idx = 1 if n &gt; 1: idx += 1 delta = 1 factor = 2 lastElement = 3 seq.append(lastElement) while idx &lt; n: nextElement = lastElement + delta if nextElement % factor != 0: factor += 1 elif factor == nextElement: seq.append(nextElement) lastElement = nextElement idx += 1 delta = 1 factor = 2 else: delta += 1 factor = 2 return seq except AssertionError as msg: print(msg) . genPrimeLi() . [2, 3, 5, 7, 11, 13, 17, 19, 23, 29] . Linked Lists . Now that we&#39;ve generated a sequence of prime numbers structured as a List, we&#39;ll generate the same sequence as a LinkedList. . class Node: def __init__(self, data=None): self.data = data self.next = None def __repr__(self): return str(self.data) . class LinkedList: def __init__(self): self.head = None def __repr__(self): node = self.head nodes = [] while node != None: nodes.append(str(node.data)) node = node.next nodes.append(&#39;empty&#39;) return &#39; &gt;&gt; &#39;.join(nodes) def append(self, data=None): node = self.head if self.head == None: self.head = Node(data) else: while node.next != None: node = node.next node.next = Node(data) def traverse(self, func): try: assert callable(func), &quot;function must be callable&quot; assert self.head != None, &quot;LinkedList is empty&quot; node = self.head while node != None: func(node.data) node = node.next except AssertionError as msg: print(msg) . ll = LinkedList() . ll.append(1) ll.append(2) ll.append(3) ll . 1 &gt;&gt; 2 &gt;&gt; 3 &gt;&gt; empty . primeLinkedList = LinkedList() for e in genPrimeLi(): primeLinkedList.append(e) primeLinkedList . 2 &gt;&gt; 3 &gt;&gt; 5 &gt;&gt; 7 &gt;&gt; 11 &gt;&gt; 13 &gt;&gt; 17 &gt;&gt; 19 &gt;&gt; 23 &gt;&gt; 29 &gt;&gt; empty . primeLinkedList.traverse(print) . 2 3 5 7 11 13 17 19 23 29 . Additional methods for LinkedList class might include insert, count, delete. There are variations of the linked list. Explore the web for the deprecated queue and other variants. . Binary Search Tree . Now that we&#39;ve generated a sequence of prime numbers structured as a LinkedList, we&#39;ll generate the same sequence as a binary search tree. A binary search tree, and a linked list are both special classes of graphs that are acyclic. . class BSTNode: def __init__(self, data=None): self.data = data self.right = None self.left = None . class BinarySearchTree: def __init__(self): self.head = None . !UNDER CONSTRUCTION, CHECK BACK FOR MORE SOON! .",
            "url": "https://nuttnice187.github.io/the-inception/2022/07/31/Data_Structures.html",
            "relUrl": "/2022/07/31/Data_Structures.html",
            "date": " • Jul 31, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Systems Thinking and Dynamical Systems",
            "content": "A dynamic system is a system whose current state generates its successive state by a rule or principle of change (the so-called evolution rule) and thus produces a trajectory in state space. All physical reality can be modeled in this way. . The Anatomy of Modeling Reality . For each instance of time, the change in the state of an observation (air particle) with respect to time is referred to as the particle&#39;s dynamics. For each instance of time, the particle&#39;s expected rate of change ($dx/dt$) is calculated as a function ($f$) of its state ($x$), time ($t$), the variables of control ($u$), and variables that cannot be controlled ($β$). In abstract, $dx/dt = f(x, t, u; β)$. . Finally, the state of an observable is not always as simple as the x, y, z coordinates in a 3D plane, as with our air particle example for Lorenz. Think of observed states of a brain, or perhaps an observed state of something really big. We have to think of an objective way to measure the state. We&#39;ll abstract these measurements as $y$, equal to a function of the state and time $g(x, t)$ give or take any error, or noise ($n$) while measuring. In abstract, $y= g(x,t) + n$. . Systems Thinking on the Job . I was first exposed to the concepts of systems thinking by way of learning project management methodologies (namely the Agile Method, the Lean practice, and devOps) for carrying out strategic initiatives within the workplace. . In devOps, developers and human operators are represented as instances, or observations of individuals, who implement tech-driven solutions to resolve the goals of operations&#39; side stakeholders. The system in which this is done is a closed loop. The instances of individuals have various goals. These conflicting goals and politicized views can make the process of bringing ideation to product (sometimes said as &quot;development to production&quot;) inefficent or downright impossible. . To engineer an efficient system of production deployment, devOps professionals modify the system environment to enhance the free flow of system actors to accomplish their goals in a continuous closed loop. In other words, they speed up or optimize the process of ideation to product. . I am familiar with these practices from my experience at a tech company called DXC and a healthcare organization called Change Healthcare. I worked at Deloitte, as well. Although several business leaders at Deloitte assured me that their organization had bought into these practices, I found them agile in name only. The initial conditions of this environment were heavily influenced by highly matrixed implementation partners from overseas. They were agile in powerpoint presentation buzzwords, but not in practice. On the contrary, they were quite averse to the idea. . I quietly thought this must be to optimize and maximize the base rate billed to the client. Is no one at Deloitte incentivized to build an efficient product delivery closed loop system? I never understood this. As a consultant, agile practices should equate to a amplified feedback loop, such that a perpetual consultant-client relationship forms naturally. . I no longer work at Deloitte. During my time, I made zero connections, I learned zero valuable skills. I was presented with zero projects I found interesting or related to the contributions I offer. I wonder if I could have done a better job at communicating my point of view, but at any rate, I mainly felt pushed to the side within a king&#39;s court that I was not comfortable navigating. I didn&#39;t feel that anyone valued my point of view on how implementation should be designed during planning. I got pretty tired of people telling me to jump in sharepoint to contribute to have all of my work replaced by someone else&#39;s authorship every time I would commit changes with no lineage documented. Teams I worked on, rejected using simple tools like git that are commonly used. I was erased from history, in this way. . I like to work with others in person. I think this work-from-home stuff has ruined my ability to navigate organizational complexity. I like spending a headachey Monday on the couch instead of the cubicle as much as the next guy, but sometimes I think it worked better knowing the rest of the company was still in a building somewhere, regardless of my location. Selfish, I know. . So, what? Who cares? Not me, back to dynamic systems. I&#39;m new to the idea of modeling them, so I thought I&#39;d give it a shot with a classic problem: modeling the chaotic dynamic system referred to as the Lorenz system. The idea here is to gain a little muscle memory around piecing together the components of a classic dynamic system, to ultimately become more confident at modeling industrial or business dynamics. Got to start somewhere, yea? Let&#39;s go! . The Lorenz System . The Lorenz System describes the chaotic behavior of observables or instances. In this case, these observables represent particles in a weather system. Given initial conditions with minute variation, the outcome of these particles although deterministic can wildly vary. This is referred to as chaotic. . Edward Lorenz, the father of chaos theory, once described chaos as “when the present determines the future, but the approximate present does not approximately determine the future.” . Lorenz first discovered chaos by accident while developing a simple mathematical model of atmospheric convection, using three ordinary differential equations. He found that nearly indistinguishable initial conditions could produce completely divergent outcomes, rendering weather prediction impossible beyond a time horizon of about a fortnight. . To model the system, Lorenz models the change in the 3D coordinates with respect to time as $dx/dt = σ(y - x)$, $dy/dt = x(ρ - z) - y$, and $dz/dt = xy - βz$. This is a system of differential equations we&#39;ll implement to determine an air particle&#39;s next position. . Check out an article I referenced from Geoff Boeing for my source of ideation. Check out a youtube video from Steve Brunton which highlights the objectives we have in working with these systems. Also, check out this youtube video clip of a popular sitcom, which highlights the importance of Lorenz&#39; findings. Who needs a degree, when you have videos from the internet and a big ego, eh? Grow up, recruiters! We live in an age where all knowledge is accessible to those who are willing to do a google search and commit their time. . Implementation . Why reinvent the wheel? I find it useful to look to openly available repositories of code for inspiration. We&#39;ll implement code sourced from and inspired by Geoff Boeing&#39;s repository of code, Lorenz-System. . If you hired a plumber for a thousand bucks and he spent a couple of days at your place to fix the john, you might could feel like your getting your money&#39;s worth based on the work duration. If that plumber&#39;s boss came through he&#39;d still charge you a thousand bucks, but it would only take 5 minutes. So what are you paying for? A solution, or an opportunity to spend a couple days with a plumber. Perceived value is interesting. . Some folks might take offense to someone using another person&#39;s code. I feel this is backward thinking, perhaps I&#39;m biased. I rely on sourcing code from an open community. Someday in the future, being paid for writing code may be a thing of the past. I&#39;ve made a lot of dough from copying and pasting, since way back to excel days. The trick is knowing what to copy and paste, a skill a bit more complex than one might imagine. . For the sake of the IP police, I rewrote Boeing&#39;s stuff as below. . %matplotlib inline import numpy as np, matplotlib.pyplot as plt, glob, os import IPython.display as IPdisplay, matplotlib.font_manager as fm from scipy.integrate import odeint from mpl_toolkits.mplot3d.axes3d import Axes3D from PIL import Image . imgPath = &#39;img&#39; # initial conditions of the system x_0 = [.1, 0, 0] # initial x, y, z coordinate position u = {&#39;sigma&#39;: 10, &#39;rho&#39;: 28} beta = 8./3. # time points s = 1 e = 60 interval = 100 timePoints = np.linspace(s, e, e * interval) # define the fonts to use for plots family = &#39;serif&#39; titleFont = fm.FontProperties(family=family, style=&#39;normal&#39;, size=20, weight=&#39;normal&#39;, stretch=&#39;normal&#39;) . def getDerivatives(x, t): xCoord, yCoord, zCoord = x dxdt = u[&#39;sigma&#39;] * (yCoord - xCoord) dydt = xCoord * (u[&#39;rho&#39;] - zCoord) - yCoord dzdt = xCoord * yCoord - beta * zCoord return [dxdt, dydt, dzdt] . def plotLorenzPNG(pos, idx): fig = plt.figure(figsize=(12, 9)) ax = fig.gca(projection=&#39;3d&#39;) ax.xaxis.set_pane_color((1,1,1,1)) ax.yaxis.set_pane_color((1,1,1,1)) ax.zaxis.set_pane_color((1,1,1,1)) x = pos[:, 0] y = pos[:, 1] z = pos[:, 2] ax.plot(x, y, z, color=&#39;b&#39;, alpha=0.7, linewidth=0.7) ax.set_xlim((-30,30)) ax.set_ylim((-30,30)) ax.set_zlim((0,50)) ax.set_title(&#39;Lorenz system attractor&#39;, fontproperties=titleFont) plt.savefig(&#39;{}/{:03d}.png&#39;.format(imgPath, idx), dpi=60, bbox_inches=&#39;tight&#39;, pad_inches=0.1) plt.close() . def getPartialTimePointsLi(timePointLi, size=20): size = max(1, size) chunks = [] for i in range(1, len(timePointLi) + 1, size): chunks.append(timePointLi[0:i]) return chunks . chunks = getPartialTimePointsLi(timePoints) . points = [] for li in chunks: points.append(odeint(getDerivatives, x_0, li)) . for i, point in enumerate(points): plotLorenzPNG(point, i) . Animation . firstLast = 10000 # ms duration for the first and last images standard = 500 # ms durations = tuple([firstLast] + [standard]* (len(points) - 2) + [firstLast]) gifName = &#39;/animation.gif&#39; # extract all the png files to an in memory list files = [] for file in glob.glob(&#39;{}/*.png&#39;.format(imgPath)): files.append(Image.open(file)) # load list to animated .gif file gif = files[0] gif.info[&#39;duration&#39;] = durations gif.info[&#39;loop&#39;] = 0 # 0 results in infinite loops gif.save(fp=imgPath+gifName, format=&#39;gif&#39;, save_all=True, append_images=files[1:]) . IPdisplay.Image(open(imgPath+gifName, &#39;rb&#39;).read()) . Conclusion . I approach learning new ideas and skills for work, somewhat like an investor, but I digress as I am still tapping my feet for the payoff! . About seven years ago as a data analyst, I couldn&#39;t shut up about PowerBI and reporting services at large. I didn&#39;t have a very big audience that was attentive. Five years later, the only jobs they would bring to me were in regards to PowerBI. Of course, I had moved on to more exciting things by then. Machine Learning became my main interest. How do we teach a computer to make predictions and more importantly for my line of work, what does the predictive model say about how we should approach changing the current state of a business process to achieve a favorable outcome. This is after all, the entropy or information that the business needs revealing. . I still can&#39;t shut up about machine learning, but like with PowerBI the adoption of the job market has not caught up yet. Like with PowerBI, the market from my point of view does not yet understand what to do with machine learning: opportunities abound. The market adopts later than I. In this way, I am early to adopt to trends. . Today, I know that machine learning is often used to discover the optimal function to describe a dynamic system, or the ideal parameter set of initial conditions to achieve a favorable outcome in a given system. I wonder if I&#39;ll start receiving more opportunities in machine learning as I drift away into other topics, like designing the formal specifications for how to control dynamic systems. . One day, I&#39;ll have to stop &quot;pioneering&quot; and sit down to cash out. At age 35, I am still in a stage of life where I have yet to experience any success. I have not secured any financial assets or property that I see as necessary for integrating into society. The grumpy, ego-centric, math nerd with a strange sense of humor deserves a place, too. Maybe there&#39;s another way. Today, I sit with those who execute for today. Maybe I should sit where they plan for tomorrow. I don&#39;t know. I am still trying to figure it out. . Today, it is trendy to secure employees for an organization in the name of diversity. Let&#39;s make sure we have a certain number of minorities in proportion to the whole. I ask, what about people like me who are just smart, sometimes not warm and fuzzy to be around? These attributes describe all of my favorite people. To me, this is desirable. . That&#39;s my personality. I am in the minority, but I don&#39;t to be a part of any diversity program. Is there a priority to retain grumpy, smart, reclusive talent? Is it appropriate for the workplace to propose that I change my personality to fit in better at work? I can only be myself. I don&#39;t want to assimilate. I love my personality. . Shifts in these ideas are a fashion or fad. I am introverted. I enjoy my time alone. You might say I am not a team player. I disagree. I add value to my team in different ways than the majority of individuals. I need to identify a project that provides an environment for my success. In the past, when I recognize that the environment I found myself in at work is not one in which I can succeed, I begin plans to checkout. . Does your place of business provide exceptions where unassimilated individuals can succeed? If I am unwilling to assimilate, the only other option seems to be an exit. I don&#39;t want to exit, beit a fair and inviting trade-off. . Perhaps there is a way to change the initial conditions, in order to understand at which time my status will change, such that I can reach my goals in serving others without sacrificing who I am. . Sound familiar? I like the idea of systems-thinking. Knock-knock. Who&#39;s there? Dynamical systems? Hello, world. .",
            "url": "https://nuttnice187.github.io/the-inception/2022/07/27/Systems_Thinking.html",
            "relUrl": "/2022/07/27/Systems_Thinking.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Neural Network from Scratch",
            "content": "Given . MNIST is an opensource dataset that contains 70,000 28 X 28 pixel sized images of hand written numbers and an accurate label for their 10 possible categorical labels of digits 0 thru 9. Each of the 784 image-pixels can be represented as a decimal between 0 and 1, where 0 would represent a blank portion of the written image and 1 would be fully saturated in writing. Therefore, each of the labeled images can be reresented as a set of 784 elements ranging between 0 and 1. . Let&#39;s import some libraries. . numpy has functions suited for working with arrays | fetch_openml will return the MNIST dataset | train_test_split simplifies the separation of historic data into datasets for training VS testing operations . a. It&#39;s important we do not use the same instances to test our model that we had previously used as ingredients to train it . | pandas is for working with dataframes | matplotlib is for drawing data visualizations | import numpy as np from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split import pandas as pd import matplotlib.pyplot as plt . Notice we are not using a data science library for purposes of fitting our model or making predictions, we look to do so from scratch. . Scope . Let&#39;s set some clear goals. . Provide formal specification of multi layer neural network functionality | Define the data processing that takes places before fitting the model | Define the class object for our MNISTNNModel | Procede to extract, transform &amp; split data, before we instantiate, train &amp; optimize the model | Test &amp; evaluate our results | Operationalize the predictive model for new predictions | Basic Functionality . A neural network is a predictive model, which functions by taking in $ vec{a}$ inputs (for us, 28 X 28 pixel-bit-depth values) and outputs a prediction for the written digit. Below, the vector of $ vec{y}$ values $y_0$ thru $y_9$ represents the final output layer. There are ten neurons in this final layer for each of the label categories. . begin{align} f_{NeuralNetwork}( begin{bmatrix} a_{0} vdots a_{783} end{bmatrix}) &amp;= begin{bmatrix} y_{0} vdots y_{9} end{bmatrix} end{align} How do we predict our most likely number label given 784 inputs? We need to have some layers of neurons between the inputs and prediction layer. Let&#39;s start with two. Our first hidden layer will have a width of 128 neuron nodes and the subsequent layer we&#39;ll have 64 neurons. Below illustrates how these neurons will interact. . Activation Functions . begin{align} σ_{sigmoid}(x)=1/(1+e^{-x}) σ_{ReLU}(x)= max(0,x) {σ prime}_{ReLU}(x)= begin{cases} 0 &amp; text{if } x &lt; 0 1 &amp; text{if } x &gt; 0 end{cases} σ_{softmax}(x)= log( sum{ exp^{x_i}}) end{align} Matrix Aggregate Computation . Hidden Layer 1 . begin{align} a^{(1)}_{n:127} &amp;= {σ}_{ReLU} begin{pmatrix} begin{bmatrix} w_{0,0} &amp; w_{0,1} &amp; dots &amp; w_{0,783} w_{1,0} &amp; w_{1,1} &amp; dots &amp; w_{1,783} vdots &amp; vdots &amp; ddots &amp; vdots w_{127,0} &amp; w_{127,1} &amp; dots &amp; w_{127,783} end{bmatrix} begin{bmatrix} a^{(0)}_{0} vdots a^{(0)}_{783} end{bmatrix} + begin{bmatrix} b_{0} vdots b_{783} end{bmatrix} end{pmatrix} a^{1} &amp;= {σ}_{ReLU}(Wa^{(0)}+b) end{align} Hidden Layer 2 . begin{align} a^{(2)}_{n:63} &amp;= {σ}_{ReLU} begin{pmatrix} begin{bmatrix} w_{0,0} &amp; w_{0,1} &amp; dots &amp; w_{0,127} w_{1,0} &amp; w_{1,1} &amp; dots &amp; w_{1,127} vdots &amp; vdots &amp; ddots &amp; vdots w_{63,0} &amp; w_{63,1} &amp; dots &amp; w_{63,127} end{bmatrix} begin{bmatrix} a^{(1)}_{0} vdots a^{(1)}_{127} end{bmatrix} + begin{bmatrix} b_{0} vdots b_{127} end{bmatrix} end{pmatrix} a^{2} &amp;= {σ}_{ReLU}(Wa^{(1)}+b) end{align} . Output Layer . begin{align} a^{(f)}_{n:9} &amp;= {σ}_{softmax} begin{pmatrix} begin{bmatrix} w_{0,0} &amp; w_{0,1} &amp; dots &amp; w_{0,63} w_{1,0} &amp; w_{1,1} &amp; dots &amp; w_{1,63} vdots &amp; vdots &amp; ddots &amp; vdots w_{9,0} &amp; w_{9,1} &amp; dots &amp; w_{9,63} end{bmatrix} begin{bmatrix} a^{(2)}_{0} vdots a^{(2)}_{63} end{bmatrix} + begin{bmatrix} b_{0} vdots b_{63} end{bmatrix} end{pmatrix} a^{f} &amp;= {σ}_{softmax}(Wa^{(2)}+b) end{align} . Preprocessing Definitions . This predictive model works best when predicting the label of one hot encoded values. The maximum bit depth for any pixel is 255. This model works best when predictors are normalized between 0 and 1. We&#39;ll define a data manipulation process to transform our inputs, as so. . def load_data(path): def one_hot(y): table = np.zeros((y.shape[0], 10)) for i in range(y.shape[0]): table[i][int(y[i][0])] = 1 return table def normalize(x): x = x / 255 return x data = np.loadtxt(&#39;{}&#39;.format(path), delimiter = &#39;,&#39;) return normalize(data[:,1:]),one_hot(data[:,:1]) . Model Definition . class MNISTNNModeler: def __init__(self, X, y, batch = 64, lr = 1e-3, epochs = 50): self.input = X self.target = y self.batch = batch self.epochs = epochs self.lr = lr self.x = self.input[:self.batch] # batch input self.y = self.target[:self.batch] # batch target value self.loss = [] self.acc = [] self.init_weights() def init_weights(self): self.W1 = np.random.randn(self.input.shape[1],256) self.W2 = np.random.randn(self.W1.shape[1],128) self.W3 = np.random.randn(self.W2.shape[1],self.y.shape[1]) self.b1 = np.random.randn(self.W1.shape[1],) self.b2 = np.random.randn(self.W2.shape[1],) self.b3 = np.random.randn(self.W3.shape[1],) def ReLU(self, x): return np.maximum(0,x) def dReLU(self,x): return 1 * (x &gt; 0) def softmax(self, z): z = z - np.max(z, axis = 1).reshape(z.shape[0],1) return np.exp(z) / np.sum(np.exp(z), axis = 1).reshape(z.shape[0],1) def shuffle(self): idx = [i for i in range(self.input.shape[0])] np.random.shuffle(idx) self.input = self.input[idx] self.target = self.target[idx] def feedforward(self): assert self.x.shape[1] == self.W1.shape[0] self.z1 = self.x.dot(self.W1) + self.b1 self.a1 = self.ReLU(self.z1) assert self.a1.shape[1] == self.W2.shape[0] self.z2 = self.a1.dot(self.W2) + self.b2 self.a2 = self.ReLU(self.z2) assert self.a2.shape[1] == self.W3.shape[0] self.z3 = self.a2.dot(self.W3) + self.b3 self.a3 = self.softmax(self.z3) self.error = self.a3 - self.y def backprop(self): dcost = (1/self.batch)*self.error DW3 = np.dot(dcost.T,self.a2).T DW2 = np.dot((np.dot((dcost),self.W3.T) * self.dReLU(self.z2)).T,self.a1).T DW1 = np.dot((np.dot(np.dot((dcost),self.W3.T)*self.dReLU(self.z2),self.W2.T)*self.dReLU(self.z1)).T,self.x).T db3 = np.sum(dcost,axis = 0) db2 = np.sum(np.dot((dcost),self.W3.T) * self.dReLU(self.z2),axis = 0) db1 = np.sum((np.dot(np.dot((dcost),self.W3.T)*self.dReLU(self.z2),self.W2.T)*self.dReLU(self.z1)),axis = 0) assert DW3.shape == self.W3.shape assert DW2.shape == self.W2.shape assert DW1.shape == self.W1.shape assert db3.shape == self.b3.shape assert db2.shape == self.b2.shape assert db1.shape == self.b1.shape self.W3 = self.W3 - self.lr * DW3 self.W2 = self.W2 - self.lr * DW2 self.W1 = self.W1 - self.lr * DW1 self.b3 = self.b3 - self.lr * db3 self.b2 = self.b2 - self.lr * db2 self.b1 = self.b1 - self.lr * db1 def fit(self): for epoch in range(self.epochs): l = 0 acc = 0 self.shuffle() for batch in range(self.input.shape[0]//self.batch-1): start = batch*self.batch end = (batch+1)*self.batch self.x = self.input[start:end] self.y = self.target[start:end] self.feedforward() self.backprop() l+=np.mean(self.error**2) acc+= np.count_nonzero(np.argmax(self.a3,axis=1) == np.argmax(self.y,axis=1)) / self.batch self.loss.append(l/(self.input.shape[0]//self.batch)) self.acc.append(acc*100/(self.input.shape[0]//self.batch)) def plot(self): plt.figure(dpi = 125) plt.plot(self.loss) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Loss&quot;) def acc_plot(self): plt.figure(dpi = 125) plt.plot(self.acc) plt.xlabel(&quot;Epochs&quot;) plt.ylabel(&quot;Accuracy&quot;) def predict(self,xtest,ytest): self.x = xtest self.y = ytest self.feedforward() acc = np.count_nonzero(np.argmax(self.a3,axis=1) == np.argmax(self.y,axis=1)) / self.x.shape[0] print(&quot;Accuracy:&quot;, 100 * acc, &quot;%&quot;) . Execution Procedure . X_train, y_train = load_data(&#39;./sample_data/mnist_train_small.csv&#39;) X_test, y_test = load_data(&#39;./sample_data/mnist_test.csv&#39;) # Declare an object as an instance of our model NN = MNISTNNModeler(X_train, y_train) # Execute training function NN.fit() . Model Evaluation . NN.plot() NN.acc_plot() . NN.predict(X_test,y_test) . Accuracy: 88.47 % . Operationalizing Predictions . Conclusion .",
            "url": "https://nuttnice187.github.io/the-inception/2022/05/28/NumpyNN.html",
            "relUrl": "/2022/05/28/NumpyNN.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://nuttnice187.github.io/the-inception/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://nuttnice187.github.io/the-inception/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Phillip A. Raywood . Analyst, Developer, Engineer . Las Vegas, Nevada . Phillip models operational outcomes as a function past observation. With this, we have a hypothetical model to understand how maintaining leading measurables at a certain level will help achieve predictable operational results. His experience is rooted in predictive modeling, prototyping of reporting tools and pre-implementation strategy experimentation on test-groups, but also reporting, business intelligence, and programming. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nuttnice187.github.io/the-inception/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nuttnice187.github.io/the-inception/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}