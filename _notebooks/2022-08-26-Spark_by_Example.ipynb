{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark by Example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCstJ2SHfEAFI2F3wGaOYD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark in Google Colab\n",
        "\n",
        "Ever want to test out Apache spark without spinning up a linux box?  Try out Colab!\n",
        "\n",
        "The entire colab runs in a cloud VM. Let's investigate the VM. You will see that the current colab notebook is running on top of Ubuntu 18.04.3 LTS (at the time of this writing.)"
      ],
      "metadata": {
        "id": "Tz37NALBkP1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# what release of linux does Colab run on?\n",
        "!cat /etc/*release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0iiaMvIi7Hn",
        "outputId": "b66a0f2b-77ad-4f2b-95e6-c9c0ed72b459"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=18.04\n",
            "DISTRIB_CODENAME=bionic\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 18.04.6 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"18.04.6 LTS (Bionic Beaver)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 18.04.6 LTS\"\n",
            "VERSION_ID=\"18.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=bionic\n",
            "UBUNTU_CODENAME=bionic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjuQaoMUgznC",
        "outputId": "000e479d-8cda-495e-cebc-bfbfb613b69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jdk-headless is already the newest version (8u342-b07-0ubuntu1~18.04).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "# install java development kit\n",
        "!sudo apt-get -y install openjdk-8-jdk-headless"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to install Apache Spark on Linux based Ubuntu, access [Apache Spark Download](https://spark.apache.org/downloads.html) site and go to the Download Apache Spark section and click on the link from ordered list item number 3, this takes you to the page with mirror URL’s to download. copy the link from one of the mirror site."
      ],
      "metadata": {
        "id": "lQhhIxqTj-23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download Apache Spark 3.0.1\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX39UZ-wiQ5p",
        "outputId": "4a5aaa52-b7d9-495e-bcfa-92ddcaaeb033"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-26 20:10:38--  https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299321244 (285M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.3.0-bin-hadoop3.tgz.1’\n",
            "\n",
            "spark-3.3.0-bin-had 100%[===================>] 285.45M  58.6MB/s    in 6.5s    \n",
            "\n",
            "2022-08-26 20:10:45 (43.8 MB/s) - ‘spark-3.3.0-bin-hadoop3.tgz.1’ saved [299321244/299321244]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip .tgz\n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "oMsKJGPamFv-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install findspark\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "bFVzc-Zwn0JK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OS module in Python provides functions for interacting with the operating system. OS comes under Python’s standard utility modules. This module provides a portable way of using operating system dependent functionality.\n",
        "\n",
        "`os.environ` in Python is a mapping object that represents the user’s environmental variables. It returns a dictionary having user’s environmental variable as key and their values as value.\n",
        "\n",
        "**`os.environ` behaves like a python dictionary, so all the common dictionary operations like get and set can be performed.** We can also modify os.environ but any changes will be effective only for the current process where it was assigned and it will not change the value permanently."
      ],
      "metadata": {
        "id": "G3Jek0Ybp65x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "UAR8Bl9Pp5EZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark isn't on `sys.path` by default, but that doesn't mean it can't be used as a regular library. You can address this by either symlinking pyspark into your site-packages, or **adding pyspark to sys.path at runtime.** findspark does the latter.\n",
        "\n",
        "To initialize PySpark, just call"
      ],
      "metadata": {
        "id": "ZOCQBbQvsxiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "Yk-BNYjJs7nR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify the automatically detected location, call"
      ],
      "metadata": {
        "id": "KArMXx-jtCa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.find()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ma6UBhDgs_El",
        "outputId": "22bd5eae-0bb3-4567-feda-35b58fea7f21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.3.0-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can `import SparkSession from pyspark`.sql and create a SparkSession, which is the entry point to Spark.\n",
        "\n",
        "You can give a name to the session using `appName()` and add some configurations with `config()` if you wish."
      ],
      "metadata": {
        "id": "siVYOaK8tSjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession\n",
        "         .builder\n",
        "         .master(\"local\")\n",
        "         .appName(\"my_colab_spark_app\")\n",
        "         .config('spark.ui.port', '4050')\n",
        "         .getOrCreate())"
      ],
      "metadata": {
        "id": "PUIYvXshtbnk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, print the SparkSession variable."
      ],
      "metadata": {
        "id": "5PXlJ1mUuxba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "qsfdDe9VueEj",
        "outputId": "84ea26f5-0ce7-4cda-f640-f5466ece9982"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff3ee87e790>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://35327839284b:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>my_colab_spark_app</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}